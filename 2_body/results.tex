We classically simulated the NVQWOA on randomly generated unlabelled graph similarity instances of size $n=8,9,10$. Each size included 30 distinct problem instances, and each was evolved for $p=10$ iterations under hyperparameters optimised for three objective functions. Across all analyses, we observed consistent amplification of optimal and near-optimal solutions, strong correlation between amplification and solution proximity to the optimum, and mixing dynamics that align with theoretical predictions for suitability. These results provide strong evidence that NVQWOA effectively explores the solution space of the graph similarity problem and concentrates probability in regions of high similarity.

STRATEGY:
First say the purpose, Second the major trends and observations, Third any key results to quantify, Fourth explain why it matters without going discussion mode.

\section{Amplification Analysis}
The first analysis aimed to verify that the NVQWOA amplifies the measurement probability of optimal and near-optimal solutions relative to the uniform initial state distribution. 

Figures \ref{fig:similarity dist} and \ref{fig:similarity log dist} show the average probability distributions over the possible similarity scores, using a linear probability scale for Figure \ref{fig:similarity dist} and a log probability scale for \ref{fig:similarity log dist}. The post-amplification distributions are skewed towards higher-similarity solutions compared to the initial uniform distribution.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{similarity_distributions_multiple.png} 
    \caption{Probability distributions over the possible similarities, demonstrating numerical support for amplification of optimal and near-optimal solutions. 30 problem instances of randomly generated ER graphs with probability of edge inclusion 0.5, with $p=10$ iterations of the NVQWOA applied.}
    \label{fig:similarity dist}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{log_similarity_distributions_multiple.png} 
    \caption{Depicts the same distributions as Fig \ref{fig:similarity dist} with a logarithmic y-axis.}
    \label{fig:similarity log dist}
\end{figure}

Figure \ref{fig:osp} is a boxplot showing the probability of measuring the optimal solution (OSP) after amplification. The OSP varied significantly across problem instances but consistently improved following NVQWOA evolution. Median OSP values increased by more than an order of magnitude over the initial probability when using the expected value as the objective function. When

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{OSP_boxplot_multiple.png} 
    \caption{Optimal solution probability (OSP) distributions. The average upper bound represents the mean OSP over all problem instances after optimising the hyperparameters to maximise the OSP.}
    \label{fig:osp}
\end{figure}



\section{Hamming Distance Analysis}
We next analysed the Hamming distance between each solution and the optimal solution(s) to determine whether amplification favours solutions close to the optimum in the solution space.

Figure \ref{fig:avg ham} shows the mean Hamming distance before and after amplification for each problem instance.
\begin{figure}[htbp]
     \centering
     \begin{subfigure}{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{n=8_avg_hamming_distance_each_instance.png}
         \caption{$n=8$}
         \label{fig:avg ham 8}
     \end{subfigure}
     \hfill
     \begin{subfigure}{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{n=9_avg_hamming_distance_each_instance.png}
         \caption{$n=9$}
         \label{fig:avg ham 9}
     \end{subfigure}
     \hfill
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{n=10_avg_hamming_distance_each_instance.png}
         \caption{$n=10$}
         \label{fig:avg ham 10}
     \end{subfigure}
        \caption{Mean hamming distance for each problem instance.}
        \label{fig:avg ham}
\end{figure}

As shown in Figure \ref{fig:ham improvement}, the mean Hamming distance decreased significantly after amplification, with X and Y and Z as the average reductions.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{hamming_improvement_boxplot_multiple.png}
    \caption{Improvement in Hamming distance. This shows the change in expected Hamming distance after applying the NVQWOA for $p=10$ iterations.}
    \label{fig:ham improvement}
\end{figure}

Figure \ref{fig:amp vs ham} shows how much amplification was applied to each solution based on their Hamming distance. The optimal solutions received average amplifications of X,Y and Z. Unexpectedly, solutions with a Hamming distance appear to have been amplified a small amount as well, receiving on average A,B, and C.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{amplification_vs_hamming_multiple.png}
    \caption{Mean amplification applied to each solution grouped by their Hamming distance from the optimal solution.}
    \label{fig:amp vs ham}
\end{figure}



\section{Subshell Distance Analysis}
We also analysed the subshell distance between each solution and the optimal solution(s), which directly reflects the structure of the permutation mixer and the connectivity of the mixing graph.

Figure \ref{fig:avg sub} depicts the average subshell distance before and after amplification for each problem instance.
\begin{figure}[htbp]
     \centering
     \begin{subfigure}{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{n=8_avg_subshell_distance_each_instance.png}
         \caption{$n=8$}
         \label{fig:avg sub 8}
     \end{subfigure}
     \hfill
     \begin{subfigure}{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{n=9_avg_subshell_distance_each_instance.png}
         \caption{$n=9$}
         \label{fig:avg sub 9}
     \end{subfigure}
     \hfill
     \begin{subfigure}{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{n=10_avg_subshell_distance_each_instance.png}
         \caption{$n=10$}
         \label{fig:avg sub 10}
     \end{subfigure}
        \caption{Mean subshell distance for each problem instance.}
        \label{fig:avg sub}
\end{figure}

Figure \ref{fig:sub improvement} shows the difference in mean subshell distance after amplification.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{subshell_improvement_boxplot_multiple.png}
    \caption{Improvement in subshell distance boxplot.}
    \label{fig:sub improvement}
\end{figure}

Figure \ref{fig:amp vs sub} shows how much amplification was applied to each solution based on their subshell distance. These plots are monotonically decreasing. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{amplification_vs_subshell_multiple.png}
    \caption{Mean amplification applied to each solution grouped by their subshell distance from the optimal solution.}
    \label{fig:amp vs sub}
\end{figure}

Figure \ref{fig:mqg} shows the mean quality gap between the optimal solutions and the subshells. For all problem instances, the mean quality gap was a monotonically increasing function with subshell distance.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{mean_quality_gap_vs_subshell_distance.png}
    \caption{Mean quality gap vs subshell distance}
    \label{fig:mqg}
\end{figure}

Figure \ref{fig:shell variance} shows the amount of variance in the objective values for each subshell.
Lower variance indicates more phase-coherence within the shell, and therefore more potential for constructive interference with the target solution under optimisation of $t$ and $\gamma$. The maximum subshell variance was X, for subshell $h$ in problem $k$.\todo{find max variance}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{subshell_variance_multiple.png}
    \caption{Shell variance vs subshell distance}
    \label{fig:shell variance}
\end{figure}

\section{Hyperparameter Analysis}
We optimised the three NVQWOA hyperparameters $\beta, \gamma, t$ under three different objective functions applied to the final state: the expected value, the probability of measuring an optimal solution (OSP), and the conditional value at risk (CVaR).

The optimal values for $t$ decreased with system size for all objective functions. The optimal values for $\gamma$ remained constant. The optimal values for $\beta$ went on a walkabout.

The optimal values for the hyperparameters were distinct for each objective function across all problem instances.

Table of mean hyperparameters:

\begin{tabular}{c||c|c|c}
    $n=8$          & $\beta$ & $\gamma$ & $t$     \\\hline\hline
    Expected Value & 1.4(6)  & 0.20(4)  & 0.47(6) \\\hline
    OSP            & 0.79(5) & 1.00(4)  & 0.180(7)\\\hline
    CVaR           & 0.54(5) & 1.2(1)   & 0.17(1) 
\end{tabular}

\begin{tabular}{c|c|c|c}
    $n=9$          & $\beta$ & $\gamma$ & $t$     \\\hline
    Expected Value & 1.08(3) & 0.20(3)  & 0.40(6) \\\hline
    OSP            & 0.84(3) & 1.01(4)  & 0.164(7)\\\hline
    CVaR           & 0.60(3) & 1.16(9)  & 0.16(5) 
\end{tabular}

\begin{tabular}{c|c|c|c}
    $n=10$         & $\beta$ & $\gamma$ & $t$     \\\hline
    Expected Value & 1.10(3) & 0.21(2)  & 0.336(4)\\\hline
    OSP            & 0.85(7) & 1.02(5)  & 0.16(4) \\\hline
    CVaR           & 0.58(2) & 1.28(5)  & 0.124(6)
\end{tabular}
