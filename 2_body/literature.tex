\section{Combinatorial Optimisation Problems}\label{sec:cop}
Combinatorial optimisation problems consist of a discrete set of feasible solutions, which can be represented as a set $S$, with finite order $N=|S|$. Each of the $N$ solutions $\mathbf{x} \in S$ can be represented as a vector of $n$ variables $\mathbf{x}=(x_1, x_2,...,x_n)$, where $x_j\in\{0,1,...,k-1\}$ are called the \textit{decision variables}. The solution space $S$ may contain all possible solutions of this form (of which there are $N=k^n$) or $S$ may be restricted to solutions which satisfy some constraint (such as in the case of permutation-based problems where $k=n$ and $N=n!$). Each solution has an associated objective value (or cost), given by a objective function (or cost function) $f:S\rightarrow\mathbb{R}$, which gives a measure of the quality of any solution. The goal of the optimisation problem is to find the \textit{optimal solution} $\mathbf{x}^*$ that maximises (or minimises) the objective (or cost) function:
$$\mathbf{x}^* = \arg\min_{\mathbf{x} \in S} f(\mathbf{x})
\;\text{or}\;
\mathbf{x}^* = \arg\max_{\mathbf{x} \in S} f(\mathbf{x})$$
The structure of the objective function is determined by the problem definition. The problem definition consists of a series of criteria which are defined on a subset of the variables in $\mathbf{x}$, and the objective function conveys the extent to which a solution satisfies the criteria.

Combinatorial optimisation problems are often NP-hard, meaning that no known polynomial-time algorithm can exactly solve all instances of the problem. Examples of NP-hard combinatorial optimisation problems include the travelling salesman problem and the knapsack problem.

The main challenge in solving these problems is that they often require a near-exhaustive search of the solution space, which usually grows exponentially or super-exponentially with problem size. An example of this super-exponential growth will be present in the graph similarity problem in section \ref{graph_sim_section}.

Research in combinatorial optimisation algorithms showing exponential speedup includes:
\begin{itemize}
    \item Exact algorithms that find optimal solutions for special problem instances with certain properties. TODO: CITATIONS
    \item Approximate algorithms that find solutions which are near the optimal solutions for general problem instances.
\end{itemize}

Approximate algorithms are the focus of this thesis. TODO: EXPAND

To quantify how close a solution $\mathbf{x}_i$ is to the optimal solution, the approximation ratio $A$ is used:
$$A_i = \frac{f(\mathbf{x}_i)}{f(\mathbf{x}^*)}$$
This value compares the objective value of a solution against the maximum possible objective value. A successful approximate combinatorial optimisation algorithm should find solutions which give approximation ratios close to 1.

\section{Graph Similarity Problem}\label{graph_sim_section}
The graph similarity problem involves determining how similar two graphs are to each other. This can be useful in various applications, such as social network analysis, bioinformatics, and pattern recognition. 

There is a need to distinguish between the graph similarity problem and the graph isomorphism problem.

A pair of graphs $G_1$ and $G_2$ are considered isomorphic ($G_1\cong G_2$) if and only if there exists a bijection between their vertex sets such that any two vertices in $G_1$ are adjacent if and only if their corresponding vertices in $G_2$ are also adjacent. In this definition, the graphs are understood to be undirected, non-labelled, non-weighted graphs. However, isomorphism may apply to all other notions of graph by adding requirements to preserve any additional structures.

The graph isomorphism problem is then the task of correctly creating that bijection (or equivalently, proving that one exists). Algorithms that solve this problem are trivially verifiable, as checking the correctness of the solution only involves checking that the vertex bijection correctly preserves the edges. Since the graph isomorphism problem can be posed as a `yes-no' question on a set of inputs values, it is called a decision problem.

In contrast, the graph similarity problem is the problem of calculating how close a pair of graphs are to being isomorphic. This cannot be posed as a `yes-no' problem, as many magnitudes of similarity must meaningfully captured. There are multiple measures of graph similarity. Some examples include:

\begin{itemize}
    \item \textit{Graph Edit Distance (GED):} Counts how many `edits' are required to transform one graph into another, where `edits' include inserting/deleting a vertex or edge, or substituting one vertex or edge with another. The lower the distance, the more similar the two graphs are. TODO: CITATIONS
    \item \textit{Maximum Common Subgraph (MCS):} Finds the largest induced subgraph that is common to the two given graphs. The larger the subgraph, the more similar the two graphs are.
    \item \textit{Frobenius Distance:} Calculates the distance between two graphs using the difference between their adjacency matrices. The lower the distance, the more similar the two graphs are.
    \item \textit{Edge Overlap:} Finds the proportion of matching entries in the adjacency matrices of the graphs. The greater the proportion, the more similar the two graphs are.\cite{QAOA_graph_sim}
\end{itemize}

The choice of measure often depends on the specific application and the properties of the graphs being compared. Considerations such as cost, time, and interpretability inform which measure is best for a practical problem.

Koutra et al. \cite{deltacon} formalised a set of axioms that graph similarity measures should conform to: TODO: MAPPING OUTPUTS TO (0,1].
\begin{enumerate}
    \item Identity: $sim(G_{1},G_{2})=1 \iff G_{1} \cong G_{2}$ \label{axiom:1}
    \item Symmetric:  $sim(G_{1},G_{2})=sim(G_{2},G_{1})$ \label{axiom:2}
    \item Zero: For a clique graph $G_{1}$ and a empty graph $G_{2}$, 
    $\lim_{n \to \infty} sim(G_{1},G_{2}) = 0$ \label{axiom:3}
\end{enumerate}

The measures listed above can all be minimally adapted to satisfy these axioms.

Most graph similarity measures assume a given mapping between the vertices of the graphs. Unlike in graph isomorphism, this vertex mapping doesn't need to be a bijection, since there exists a notion of similarity between graphs with a different number of vertices. For graph similarity methods such as those listed previously, the choice of vertex mapping can change the final result. However, Koutra's axiom \ref{axiom:1} requires that graph similarity measures can identify isomorphic graphs by giving them a similarity of 1.

So to satisfy Koutra's axioms, we must find a mapping between the graphs that maximises their similarity.

To find the best mapping between labelled graphs, one can simply `pair up' the vertices in the graphs so that each pair consists of one vertex from each graph that share the same label. This significantly reduces the number of possible mappings (often to just 1), so finding the mapping with the maximum similarity is simple.

To find the best mapping between unlabelled graphs is very challenging. In the general case, the similarity associated with every possible permutation of mappings must be checked until either a mapping that shows isomorphism (or another verifiable global maximum) is found, or the entire space of mappings has been searched.

As such, the unlabelled graph similarity problem can be defined in terms of a combinatorial optimisation problem.

\subsection{Problem Definition}
A formal definition for the unlabelled graph similarity problem is as follows:

Let $G_1=(V_1,E_1)$ and $G_2=(V_2,E_2)$ be two finite, simple graphs and considered \textit{unlabelled} in the sense that vertex identifiers carry no intrinsic meaning.

Let $\Pi(V_1,V_2)$ denote the set of all maps $\pi: V_1 \to V_2$.

Given a base similarity function $f:\mathcal{G}\times\mathcal{G}\to \mathbb{R}$ that compares labelled graphs from the class $\mathcal{G}$ of all finite simple graphs, the \textit{unlabelled similarity} between $G_1$ and $G_2$ is then defined as

$$\text{Sim}(G_1,G_2)= \max_{\pi\in\Pi(V_1,V_2)} f(G_1, \pi(G_2)),$$

where $\pi(G_2)$ denotes the relabelling of $G_2$ induced by $\pi$.

The unlabelled graph similarity problem is the problem of computing $\text{Sim}(G_1,G_2)$. Since $\Pi$ is set of maps between two finite sets, the size of the set $|\Pi|$ scales super-exponentially. In the case that $|V_1|=|V_2|$ and the maps $\pi$ denote permutations of $n$ vertices (), $|\Pi|=n!$.

This problem is known to be NP-hard, meaning that there is no known polynomial-time algorithm to solve it for all instances. As a result, various heuristic and approximation techniques have been developed to tackle the graph similarity problem. This thesis will explore an approximate quantum algorithm, described in the remainder of this chapter.

\section{Quantum Computing}
Quantum computing is based on the principles of quantum mechanics, which describe the behaviour of particles at the atomic and subatomic levels.

This section provides a brief overview of the fundamental concepts of quantum computing that are relevant to understanding the non-variational QWOA and its application to the graph similarity problem. 

\subsection{Qubits and Quantum Superposition}

The fundamental unit of information in classical computing is the bit, which can exist in one of two states, typically written as 0 or 1.

In quantum computing, the basic unit of information is the quantum bit or qubit. Unlike classical bits, which can only be in one state at a time, qubits can exist in a superposition of states.

Qubits are physically realised with two-state quantum systems, such as the spin of an electron in which the two states can be taken as spin up or spin down. Qubits are mathematically described by their two-dimensional complex state vectors. Systems of qubits are described in a $2^n$ dimensional complex vector space, where $n$ is the number of qubits. A qubit's state (or the state of a collection of qubits) can be represented as the complex weighted sum of its basis states. A common choice of basis states is called the standard or computational basis, consisting of the positive and negative eigenvalues of the Pauli Z operator, commonly notated as $\ket{0}$ and $\ket{1}$, respectively. A general state of a single qubit can be expressed in the standard basis as:

$$\ket{\psi} = \alpha\ket{0} + \beta\ket{1}$$

where $\alpha$ and $\beta$ are complex numbers that satisfy the normalisation condition $|\alpha|^2 + |\beta|^2 = 1$. The Born rule states that the coefficients $|\alpha|^2$  and $|\beta|^2$ represent the probabilities of measuring the qubit in the states $\ket{0}$ and $\ket{1}$, respectively.

When multiple qubits are combined, their joint state is represented by the tensor product of their individual states. For example, the state of a two-qubit system can be expressed as:
\begin{align*}
\ket{\psi_{12}} &= \ket{\psi_1} \otimes \ket{\psi_2} = (\alpha_1\ket{0} + \beta_1\ket{1}) \otimes (\alpha_2\ket{0} + \beta_2\ket{1})\\
&=\alpha_1\alpha_2\ket{0}\ket{0} + \alpha_1\beta_2\ket{0}\ket{1} + \beta_1\alpha_2\ket{1}\ket{0} + \beta_1\beta_2\ket{1}\ket{1}\\
&= \alpha_1\alpha_2\ket{00} + \alpha_1\beta_2\ket{01} + \beta_1\alpha_2\ket{10} + \beta_1\beta_2\ket{11}
\end{align*}
where $\ket{00}, \ket{01}, \ket{10},$ and $\ket{11}$ are the basis states of the two-qubit system.

This allows a quantum computer with $n$ qubits to represent $2^n$ states simultaneously, which is a key feature that enables quantum parallelism, allowing quantum computers to perform certain computations more efficiently than classical computers.

Despite this parallelism, the computational power is limited by the fact that measuring a quantum state collapses it to one of its basis states, yielding only a single outcome. Therefore, quantum algorithms must be carefully designed to manipulate the amplitudes of the quantum states such that the desired outcomes have higher probabilities of being measured.

\subsection{Quantum Gates, Entanglement}

Quantum gates are the building blocks of quantum circuits, analogous to classical logic gates. They manipulate the states of qubits through unitary transformations, which are reversible operations that preserve the total probability of the quantum system. Common quantum gates include the Hadamard gate, which creates superposition, and the CNOT gate, which entangles two qubits. 

We can visually represent a series of quantum gates acting on qubits using a quantum circuit diagram. For example, the following diagram shows a simple quantum circuit with two qubits and two gates:

$$\begin{quantikz}
    \lstick{\ket{0}} & \gate{H} & \ctrl{1} & \meter{} \\
    \lstick{\ket{0}} & \qw      & \targ{}  & \meter{}
\end{quantikz}$$

This diagram represents a circuit where the first qubit is put into superposition by the Hadamard gate (H), and then a CNOT gate acts on the second qubit depending on whether the first qubit is in a \(\ket{1}\) state or the \(\ket{0}\) state. Finally, both qubits are measured. 

The quantum state begins as:
$$\ket{\psi}=\ket{00}$$
Then after the Hadamard gate is applied, the quantum state is:
$$\ket{\psi_H}=\frac{1}{\sqrt{2}}(\ket{00} + \ket{10})$$
Finally the CNOT gate is applied and the quantum state is:
$$\ket{\psi} = \frac{1}{\sqrt{2}}(\ket{00} + \ket{11})$$
The measurements can be performed relative to any basis in the qubit's vector space. Commonly, measurements are performed in the same basis as that is used to describe the qubits.

This state is an example of entanglement, a unique quantum phenomenon where the states of two or more qubits become correlated such that the state of one qubit cannot be described independently of the state of the other qubit(s). For example, in the entangled state above, if one were to measure the first qubit to be in the state $\ket{0}$, that necessarily requires the second qubit to also be in the state $\ket{0}$. 

The measurement of one of the entangled qubits has collapsed the superposition for both qubits. This surprising result was initially deemed unphysical on the grounds that it violates local realism by Einstein, Poldowsky, and Rosen, the first to describe this behaviour\cite{epr_paradox}. Nevertheless, quantum entanglement and its implications have been repeatedly verified through experimentally violating Bell's inequality\cite{bell_theorem}, establishing that the correlations produced from quantum entanglement cannot be explained by local hidden variables\cite{bell_inequality_1,bell_inequality_2,bell_inequality_3}.
These correlations are fundamental to quantum algorithms. Without entanglement, quantum computers could be efficiently simulated by classical computers, negating any advantage\cite{Jozsa_2003}.

\subsection{Quantum Algorithms and Challenges}

Quantum algorithms seek to leverage properties of qubits and quantum gates to solve specific problems more efficiently than classical algorithms. Notable examples include Shor's algorithm for integer factorisation\cite{Shor_algorithm}, which runs exponentially faster than the best-known classical algorithms, and Grover's algorithm for unstructured search\cite{grover_search}, which provides a quadratic speedup over classical search algorithms.

Large, universal, fault-tolerant quantum computers that can run these algorithms at useful scales will require large numbers of qubits. While researchers have made significant progress in error correction, coherence times, qubit connectivity, and gate fidelities, fault tolerant quantum computing may take decades to achieve. In the meantime, Noisy Intermediate-Scale Quantum (NISQ) computers already exist, and can run small-scale quantum algorithms that can provide insights into quantum behaviour and inform the development of larger systems.

These NISQ devices are limited by the number of qubits, gate fidelities, and coherence times. As a result, quantum algorithms designed for NISQ devices must be efficient in terms of qubit usage and circuit depth to minimise the impact of noise and errors. 

Even if NISQ devices cannot solve problems faster than the best classical computers using the best classical algorithms (particularly with the emergence of exascale supercomputing), they could still provide an economic advantage by using less energy or by being lower cost to build and operate.

NISQ algorithms are designed to work within these low-resource constraints. These algorithms typically involve a hybrid approach, where a quantum computer is used to prepare and measure quantum states, while a classical computer optimises parameters based on the measurement results. Some examples of NISQ-era algorithms are described in the following sections.

\section{Variational Quantum Algorithms}
\subsection{VQAs in General}
Variational Quantum Algorithms (VQAs) are a class of hybrid quantum-classical algorithms that leverage the strengths of both quantum and classical computing to solve optimisation problems. They are particularly well-suited for NISQ devices due to their relatively low resource requirements and robustness to noise.

VQAs implement a parametrised quantum circuit (PQC) whose unitary evolution is governed by a set of $d$ classically tunable parameters $\mathbf{\theta}\in\mathbb{R}^d$. The quantum device prepares the state
$$\ket{\psi(\mathbf{\theta})}=U(\mathbf{\theta})\ket{0}^{\otimes n}$$
where $U(\mathbf{\theta})$ is a depth-restricted circuit composed of parameterised single- and multi-qubit gates, and $n$ is the number of qubits in the input to the quantum computer. Commonly, $d=\mathcal{O}(\text{poly}(n))$, since we seek efficient circuits that are polynomial in resources. TODO: CITATION

The objective function of the algorithm is typically expressed as the expectation value of a Hermitian operator $H$ with respect to the variational state:
$$C(\mathbf{\theta})= \bra{\psi(\mathbf{\theta})}H\ket{\psi(\mathbf{\theta})} $$

This cost function is estimated on the quantum device via repeated projective measurements in appropriate bases. A classical optimiser updates $\mathbf{\theta}$ with the aim of minimising (or maximising) $C(\mathbf{\theta})$, thus driving the quantum circuit towards a state that approximates the optimal configuration with respect to the target problem. The algorithm repeatedly prepares this variational quantum state, measures its expectation value, and optimises the parameters until a certain number of iterations is complete or until measurements of the quantum state have an approximation ratio sufficiently close to 1.

To summarise, VQAs consist of a two-step loop: a quantum computer evolves the initial state to a final state via a PQC, then a classical computer measures the expectation value of the final state and uses this data to update the PQC. At the end of this loop, the quantum computer's output is now a good approximation of the optimal configuration.

\subsection{QAOA}
The Quantum Approximate Optimisation Algorithm (QAOA) is a subclass of VQAs designed to solve combinatorial optimisation problems, introduced by Farhi et al. \cite{QAOA} In this formulation, the PQC is defined by alternating applications of unitaries. One is a quality mixer associated with the problem instance and the other is a mixing operation that induces interference.

The quantum state begins in the equal superposition state over the computational basis states:
$$\ket{s}=\frac{1}{\sqrt{2^n}}\sum_{z}\ket{z}$$
where $n$ is the number of qubits.

Define a unitary operator $U(\gamma)$ which depends on: an angle $\gamma$, which lies between 0 and $2\pi$; and a quality operator $Q$, which is a diagonal operator that contains the objective function value for each solution.
$$U(\gamma)=e^{i\gamma Q}$$
Define operator $U(\beta)$ which depends on the parameter $\beta$, which lies between 0 and $\pi$.
$$U(\beta)=\Pi_{j=1}^{n} e^{-i\beta \sigma_j^x}$$
Here, $\sigma_j^x$ denotes the Pauli X operator applied to the $j$ register. This is the mixing unitary, which induces interference between related solutions. Other mixers are introduced for use in the NVQWOA in section \ref{sec:NVQWOA}.

Now, for any integer $p\geq1$, and $2p$ angles $\mathbf{\gamma}=(\gamma_1, ...,\gamma_p)$ and $\mathbf{\beta}=(\beta_1, ...,\beta_p)$ we define the final quantum state:
$$\ket{\mathbf{\gamma},\mathbf{\beta}}=U(\beta_p)U(\gamma_p)\dots U(\beta_1)U(\gamma_1) \ket{s}$$
This quantum state is acquired by repeatedly applying $U(\gamma)$ then $U(\beta)$. The action that $U(\gamma)$ has on each individual state is to apply a local phase shift to its complex amplitude proportional to the quality of the solution represented by that state. Because this results in the phases of the solutions being separated from one another, this unitary is called the phase separation unitary. The action that $U(\beta)$ has on each individual state is equivalent to performing a quantum walk on a graph representing the connections between different states.

We can now do the usual VQA method of measuring the expectation value, optimising the parameters $\mathbf{\gamma}$ and $\mathbf{\beta}$, and repeating to obtain good approximations of the optimal solution.

\subsection{Challenges}
Although VQAs are a promising platform for NISQ computing, their effective performance relies on the computationally expensive tuning of an often prohibitively large number of variational parameters. Additionally, the performance of these variational algorithms is highly dependent on the initialisation of the variational parameters.

Therefore, it can be argued that VQAs replace one difficult optimisation problem with another difficult optimisation problem.

\section{Non-Variational Quantum Walk-based Optimisation Algorithm}\label{sec:NVQWOA}
\subsection{Overview}
The NVQWOA is a novel algorithm introduced by Bennett et al. that builds upon and addresses the variational challenges of the QAOA\cite{bennett2024nonvariational,bennett2024analysisnonvariationalquantumwalkbased}. 
Like the QAOA, it alternates between applying two unitaries: a phase-separation unitary that encodes the objective function into the complex phase of the states and a mixing unitary that induces quantum interference between the solutions. Unlike QAOA or other VQAs, the parameters defining the unitaries are not variationally tuned by a classical optimiser. Instead, they are predetermined by the choice of only three hyperparameters $\gamma, t, \beta$. Additionally, the structure of the mixer is chosen to match the structure of the problem, to connect nearest-neighbour solutions to one another.

Simulations have demonstrated that this non-variational approach yields rapid convergence to global optima across diverse problem types, including MaxCut, maximum independent set, k-means clustering, capacitated facility location, and the quadratic assignment problem, using only modest circuit depths and a handful of iterations \cite{bennett2024analysisnonvariationalquantumwalkbased}.

\subsection{Algorithm Description}
Using the terms and definitions of combinatorial optimisation problems from Section \ref{sec:cop}:

The algorithm begins with an initial superposition over all possible solution states:
$$\ket{s}=\frac{1}{\sqrt{N}}\sum_{\mathbf{x}\in S}\ket{\mathbf{x}}$$

Each iteration of the NVQWOA applies two unitaries:
$$U_Q(\gamma)=e^{-i\gamma Q},\;\;U_M(t)=e^{-itA}$$
where $Q$ is a diagonal operator such that $Q\ket{\mathbf{x}}=f(\mathbf{x})\ket{\mathbf{x}}$ encodes the objective function values, and $A$ is the adjacency matrix of a mixing graph whose vertices correspond with feasible solutions and whose edges connect nearest-neighbour solutions to one another.

The phase-separation unitary applies a phase to each solution states, proportional to the objective value of that solution $f(\mathbf{x})$, while the mixing unitary $U_M(t)$ implements a continuous time quantum walk (CTQW) on the mixing graph, redistributing complex amplitudes through neighbouring solutions. Repeated alternating application of these unitaries produces a designed interference process in which amplitudes associated with optimal or near-optimal solutions interfere constructively, amplifying their measurement probabilities.

The final quantum state after $p$ iterations of each unitary is
$$\ket{\gamma,t,\beta}=\left[\prod_{i=0}^{p-1} U_M(t_i)U_Q\left(\frac{\pm \gamma_i}{\sigma}\right)\right] \ket{s}$$
where the $\pm$ accounts for whether the goal is minimisation ($-$) or maximisation ($+$), $\sigma$ is the standard deviation of the initial objective function landscape (which can be efficiently approximated through random sampling of $p=0$), $t_i$ and $\gamma_i$ are obtained from specific formulae in terms of $\gamma, t,$ and $\beta$ and where $\gamma>0, t>0,$ and $0<\beta<1$:
$$\gamma_i=\left(\beta + (1-\beta)\frac{i}{p-1} \right)\gamma,\;\;t_i=\left(1- (1-\beta)\frac{i}{p-1} \right)t$$
So the angles $\gamma_i$ increase linearly with $i$ and the mixing times $t_i$ decrease linearly with $i$.

\subsection{Mixing Unitary}
A key conceptual advance of the NVQWOA lies in the explicit construction of a problem-structured mixing graph (also referred to as mixing unitary or mixer). The vertices of the graph represent feasible solutions, and edges connect nearest neighbour configurations, typically those separated by a minimal Hamming distance or by a single elementary move. The resulting adjacency matrix $A$ defines a vertex-transitive graph whose degree $d$ is polynomial in problem size.

\subsubsection{Mixer Examples}

In his work, Bennett provides some examples of mixing graphs with known efficient implementations on a quantum computer \cite{bennett2024nonvariational,bennett2024analysisnonvariationalquantumwalkbased}:

\textit{Binary Mixer}: For problems with $n$ binary decision variables $x_j\in\{0,1\}$, the mixing graph is a $n$-dimensional hypercube, which is identical to the transverse-field Hamiltonian from the QAOA. Solutions are connected to the $n$ other solutions which differ by one bit-flip.

\textit{Integer Mixer}: For problems with $n$ integer decision variables $x_j\in\{0,1,...k-1\}$. Solutions are connected to the $n(k-1)$ other solutions which differ in the choice of one decision variable.

\textit{Permutation Mixer}: For problems with $n$ integer decision variables $x_j\in\{0,1,...k-1\}$ where the solutions are constrained to not have repeated decision variables. Solutions are connected to the $\frac{1}{2}n(n-1)$ other solutions which are transpositions of each other.

The choice of mixing graph directly determines how probability amplitudes are propagated, making it possible to exploit problem structure, unlike the transverse-Hamiltonian mixer in the QAOA, which applies the same mixer to all combinatorial optimsiation problems\cite{QAOA}.

\subsubsection{Mixer Subshells}\label{sec:subshell}
The distance $dist(u,v)$ between two vertices $u,v$ on a graph is the minimum length of the paths connecting them. A subshell is a subset of the vertices of the mixing graph, defined in relation to a reference vertex $u$. The subshell $h_u$ contains all vertices that are a distance $h$ away from $u$. The set of all subshells for a graph form equivalence classes on the set of the vertices of the graph.

For the NVQWOA to perform well on a combinatorial optimisation problem, it is expected to satisfy two conditions. First, the action of the mixer on an arbitrary solution state $\ket{\mathbf{u}}$ must be as follows:
$$
U_M(t)\ket{\mathbf{u}}=\sum_{h=0}^D \left(e^{-ih\phi(t)} \sum_{\mathbf{x}\in h_{\mathbf{u}}} r_\mathbf{x}(t)\ket{\mathbf{x}} \right),
$$
where $D$ is the diameter of the mixing graph and (for sufficiently small $t$) $r_\mathbf{x}$ and $\phi$ are positive real-valued functions, and $0<\phi(t)<\pi$. In essence, probability amplitude from an initial vertex is distributed to other vertices such that the complex phase of the distributed probability amplitudes are proportional to their distance from the initial vertex.

The second condition is that the following relationship should be at least approximately satisfied,
$$
(\mu_{h\mathbf{x}}-f(\mathbf{x}))\approx -m_h (f(\mathbf{x})-\mu),
$$
where $\mu_{h\mathbf{x}}$ is defined as the mean objective function value of solutions contained in $h_{\mathbf{x}}$, $\mu$ is the mean objective function value of all solutions in $S$, and $m_h$ is a positive constant. Also, the proportionality $m_h$ should increase monotonically with increasing $h$ (up to a distance which is a considerable fraction of the graph's diameter $D$)